\section{Introduction}

\subsection*{Define object localization and applications}

Localizing livestock individuals from images or videos has became an essential task in precision livestock farming (PLF) \citep{}. Such techniques allows researchers and farm managers to monitor the health and well-being of animals in real-time, optimizing their resource management and improving sustainability \citep{}. Technically speaking, in the field of computer vision (CV), which is a subfield of artificial intelligence (AI) that focuses on translating visual information into actionable insights, the localization tasks can be further categorized into object detection, object segmentation, and pose estimation. Object detection is the simplest form of localization, which localizes objects of interest by enclosing them within a rectangular bounding box defined by x and y coordinates, pixel width, and pixel height. <examples 1, 2>. To have a finer localization, object segmentation is deployed to outline the object contours pixel-wise, while pose estimation is achieved by orienting and marking the keypoints of the object. <another examples 3, 4>.

\subsection*{Model generalization, pre-training, and fine-tuning}

Although implementing image-based systems in the livestock production has shown promising results, current studies merely focus on the accuracy on homogenous environments and rarely address the challenges of model generalization. Model generalization refers to how well a model can perform on unseen data, which is crucial when ones want to reproduce existing studies or models in their own environments. The generalization of a CV model can be affected by a variety of factors in the deployment environment, such as camera angles and the presence of occlusions. Deploying the same model in a new environment with different conditions cannot necessarily guarantee the same performance as reported in the original study. \cite{} Li2021 also pointed out that lightning condition of farms in real applications can be highly variable, leading a poor generalization th new environments.

One explaination for the poor generalization is the discrepancy between the pre-training process and the specific use case. Most CV models are released with pre-trained weights, which were obtained from the results of training on a large-scale dataset. For example, the COCO dataset \citep{} is a general-purpose dataset that contains more than 200 thousands images and a wide range of object categories, such as vehicles and household items. Directly deploying a model pre-trained on the COCO dataset to specifically detect cows in a farm setting may not ensure satisfactory performance, as the dataset does not contain enough cow instances in different view angles or occlusions. To alleviate the discrepancy, fine-tuning is a common practice that modifies the prediction head of the pre-trained model and updates the weights on a new dataset that is more relevant to the specific use case. Most application studies have adopted this approach to improve the model generalization on their specific tasks (examples 1-5).

Nevertheless, the fine-tuning is not guaranteed to be successful, as the outcome depends on both the quantity and quality of the annotated dataset. For example, zin et. al.(2020) deployed an object detection model to recognize cow ear tags in a dairy farm. Although the model achieved a high accuracy of 92.5\% on recognizing the digits on the ear tags, more than 10 thousand images were required for fine-tuning the model. Assemblying such a large dataset is labor-intensive and requires specific training in annotating the images. Because the annotated dataset is rigorously organized in specific format. For example, the COCO annotation format \citep{} store the image information, object class, and annotations of the entire dataset in one nested JSON format \citep{}. Whereas the YOLO format \citep{}, another common format for object localization, stores information of one image in one text file, with each line representing one object instance in the image. Additionally, unlike the COCO format that stores bounding box coordinates in absolute pixel values, the YOLO format stores the coordinates in relative values to the image size. These technical details are keys to valid annotations, which are usually helped by the professional annotation tools such as labelme \citep{}, CVAT \citep{}, or Roboflow \citep{}.

\subsection*{Model Complexity and Performance}

Another perspective that affects the model generalization is model complexity. In general, model complexity is quantified by the number of learnable parameters in a model \citep{}. A more complex model often can better generalize to unseen data with higy accuracy. However, such high complexity also comes with a cost of computational resources in a form of either memory or time \citep{}. The computational cost may further limit how the models can be deployed in real-world applications, where real-time processing or edge computing is desired for fast or compact systems. For instance, the VGG-16 model {simonyan2014very} has 138 million parameters and recommends a video memory of at least 8GB, while the ResNet-152 {he2016deep} has around 60 million parameters with a recommended video memory of 11GB. Additionally, recent models for object detection such as YOLOv8 \citep{} and YOLOv9 \citep{} have been developed in different sizes and therefore provide a flexible choice for researchers to balance between the generalization performance and the computational cost. In YOLOv8, the spectrum of model complexity ranges from the highly intricate, such as YOLOv8x containing 68.2 million parameters, to more streamlined variants YOLOv8n with only 3.2 million parameters. And the demand for the memory, solely from the model architecture without considering the intermediate results during the training or inferenecne process, is larger in a factor of 21 for YOLOv8x (136.9 megabytes) compared to YOLOv8n (6.5 megabytes). Therefore, the trade-off between the model complexity and the computational cost is a critical factor to consider in deploying CV models in real-world scenarios.

\subsection*{YOLO Models}

\subsection*{Public Datasets}

A public dataset helps the community to develop methodology based on the same baseline. One famous example in computer vision is the ImageNet dataset \citep{}, which serves as a benchmark for image classification. AlexNet \citep{}, the winner of the ImageNet Large Scale Visual Recognition Challenge in 2012, show its outstanding capability to classify images in ImageNet dataset using Rectified Linear Units (ReLU) as the activation function than the traditional sigmoid function. The success of AlexNet accelerate the developement of CV models in the following years, such as VGG \citep{}, GoogLeNet \citep{}, ResNet \citep{}, and DenseNet \citep{}. However, similar to the challenges that pre-trained models face in the specific use case, a generic public dataset, such as ImageNet and COCO, may not be sufficient to PLF applications. There  were efforts to create public datasets for livestock scenario. For example, XXX \citep{} was collected for xxx. Another example in pigs xxx.

\subsection*{Study Objectives}

This study aims to explore model generalization across varying environmental settings and model complexities within the context of indoor cow localization. It seeks to exmaine three practical hypotheses:

\begin{itemize}
    \item \textbf{Model generalization is equally influenced by changes in lighting conditions and camera angles.} Should camera angles prove more impactful than lighting conditions, it would be advisable to prioritize camera placement when deploying computer vision (CV) models in new environments.
    \item \textbf{There is a positive correlation between model complexity and generalization performance.} If a highly complex model does not ensure superior performance, future studies might consider adopting less computationally demanding models that still enhance performance.
    \item \textbf{The choice of the initial weights has no significant impact on the model's generalization performance.} If the weights obtained from the fine-tuning process throught a similar context do not improve the transfer learning to the new environment in comparison to the pre-trained weights, the fine-tuning effort may be deemed unnecessary for certain tasks.
\end{itemize}

To facilitate these investigations, a public dataset named COws LOcalization (COLO) will be developed and made available to the community. The findings of this study are expected to provide practical guidelines for Precision Livestock Farming (PLF) researchers on deploying CV models, considering available resources and anticipated performance.
